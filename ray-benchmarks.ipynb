{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d652999-2f4e-42e2-9d33-c816e167e78f",
   "metadata": {},
   "source": [
    "## SAMPLE - Generate AWS Session Tokens from a AWS Profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d1d0651-4019-417c-9f04-3c3b2c002f3c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import boto3\n",
    "import boto3.session\n",
    "AWS_ACCOUNT_NO=os.environ['AWS_ACCOUNT_NO']\n",
    "AWS_ROLE_NAME=os.environ['AWS_ROLE_NAME']\n",
    "AWS_ROLE_ARN=f'arn:aws:iam::{AWS_ACCOUNT_NO}:role/{AWS_ROLE_NAME}'\n",
    "os.environ['AWS_ROLE_ARN'] = AWS_ROLE_ARN\n",
    "session = boto3.session.Session()\n",
    "sts_client = session.client('sts')\n",
    "sts_client.get_caller_identity()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56fdac05-e869-4186-8dfd-e1f15312bc44",
   "metadata": {
    "tags": []
   },
   "source": [
    "## SAMPLE - Ray Worker method\n",
    "\n",
    "1. You pass the AWS Config File Contents directly to the worker\n",
    "2. The worker writes the contents to the path AWS_CONFIG_FILE\n",
    "3. The worker generates AWS Credentials based on AWS PROFILE\n",
    "4. The worker creates a 100 MB file\n",
    "5. The worker writes the 100 MB file to a custom bucket which is accessible by the chosen AWS Profile\n",
    "\n",
    "Note - The experiment is created in the workspace. It is passed directly to the worker as a parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b536c6-e115-45c2-a52a-e7d0de33446e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import ray\n",
    "import mlflow\n",
    "import boto3\n",
    "import multiprocessing\n",
    "import uuid\n",
    "import math\n",
    "import numpy as np\n",
    "bucket = os.environ['MLFLOW_ARTIFACTS_BUCKET_NAME']\n",
    "def generate_cpu_load(interval=1*10, utilization=100):\n",
    "    \"Generate a CPU utilization % for a duration of interval seconds\"\n",
    "    start_time = time.time()\n",
    "    for i in range(0,int(interval)):\n",
    "        while time.time()-start_time < utilization/100.0:\n",
    "            a = math.sqrt(64*64*64*64*64)\n",
    "        time.sleep(1-utilization/100.0)\n",
    "        start_time += 1\n",
    "        \n",
    "def get_aws_credentials(aws_role_arn):\n",
    "    os.environ['AWS_ROLE_ARN']=aws_role_arn\n",
    "    session = boto3.Session()\n",
    "    region_name=session.region_name\n",
    "    aws_access_key_id=session.get_credentials().access_key\n",
    "    aws_secret_access_key=session.get_credentials().secret_key\n",
    "    aws_session_token= session.get_credentials().token\n",
    "    return aws_access_key_id,aws_secret_access_key,aws_session_token\n",
    "\n",
    "def create_file():\n",
    "    filename = f\"/tmp/f1.txt\"\n",
    "    with open(filename, \"wb\") as f:\n",
    "        f.seek((1024 * 1024 * (i+1)*100) - 1)\n",
    "        f.write(b\"\\0\")\n",
    "    return filename\n",
    "\n",
    "@ray.remote\n",
    "def mlflow_write_remote(exp_name,idx,project_id,aws_role_arn,direct_s3_write=False):\n",
    "    key,secret,session = get_aws_credentials(aws_role_arn)\n",
    "    #MLFLOW Client needs these env variables set\n",
    "    os.environ['AWS_ACCESS_KEY_ID'] = key\n",
    "    os.environ['AWS_SECRET_ACCESS_KEY'] = secret\n",
    "    os.environ['AWS_SESSION_TOKEN'] = session\n",
    "    # Spin CPU for a bit\n",
    "    num_cpus = multiprocessing.cpu_count()\n",
    "    print(f\"Spinning CPUs (num_cpus={num_cpus})\")\n",
    "    processes = []\n",
    "    for _ in range (num_cpus):\n",
    "        p = multiprocessing.Process(target=generate_cpu_load)\n",
    "        p.start()\n",
    "        processes.append(p)\n",
    "    for process in processes:\n",
    "        process.join()\n",
    "    uuid_str = str(uuid.uuid4())\n",
    "    filename = create_file()\n",
    "    st = time.time()\n",
    "    exp = mlflow.get_experiment_by_name(exp_name)    \n",
    "    EXPERIMENT_ID = exp.experiment_id    \n",
    "    artifact_location = exp.artifact_location\n",
    "    # Generate a large (~35M) dataframe and upload it to S3\n",
    "    print(f\"Uploading stuff to S3\")\n",
    "    # TODO: REPLACE WITH CORRESPONDING S3 BUCKET URL\n",
    "    #s3_project_prefix = \"s3://<bucket_name>/mlflow/project-test/secure\"\n",
    "    s3_project_prefix = artifact_location\n",
    "    uuid_str = str(uuid.uuid4())\n",
    "    \n",
    "    df = pd.DataFrame(np.random.randint(0, 100, size=(1000000, 10)), columns=list('ABCDEFGHIJ'))\n",
    "    df.to_csv(f\"/tmp/{idx}.csv\")\n",
    "    s3_client = boto3.client('s3')  \n",
    "    uuid_str = str(uuid.uuid4())\n",
    "    \n",
    "    retry = 0\n",
    "    st = time.time()\n",
    "    \n",
    "    with mlflow.start_run(experiment_id=EXPERIMENT_ID) as run:\n",
    "        retry = 0\n",
    "        run_id=run.info.run_id\n",
    "        while(retry < 5):\n",
    "            try:\n",
    "                print(direct_s3_write)\n",
    "                if (direct_s3_write):\n",
    "                    object_name = f'mlflow/{project_id}/{run_id}/artifacts/large_files-direct/{uuid_str}.txt'\n",
    "                    print(object_name)\n",
    "                    response = s3_client.upload_file(f\"/tmp/{idx}.csv\", bucket, object_name)\n",
    "                    \n",
    "                else:\n",
    "                    mlflow.log_artifact(f\"/tmp/{idx}.csv\", artifact_path='large_files')\n",
    "                break\n",
    "            except:\n",
    "                retry = retry + 1\n",
    "        \n",
    "    end = time.time()\n",
    "    duration = str(end-st)\n",
    "    return duration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f79fb4-30d1-4cd6-8c8c-5e8cb7243db4",
   "metadata": {},
   "source": [
    "## SAMPLE - Initialize the Ray Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fcf7b22-804e-4ca4-b381-6b27dd664a6b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import ray\n",
    "import mlflow\n",
    "import boto3\n",
    "if not ray.is_initialized():\n",
    "    service_host = os.environ[\"RAY_HEAD_SERVICE_HOST\"]\n",
    "    service_port = os.environ[\"RAY_HEAD_SERVICE_PORT\"]\n",
    "    address=f\"ray://{service_host}:{service_port}\"\n",
    "    temp_dir='/mnt/data//{}/'.format(os.environ['DOMINO_PROJECT_NAME']) #set to a dataset\n",
    "    ray.init(address=address)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5481010-86b9-485e-9298-ff11c7701438",
   "metadata": {
    "tags": []
   },
   "source": [
    "## SAMPLE - Configure number of workers and start job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e53988-1480-4e9f-acf7-ebe63b2b7ec5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "no_of_workers=1\n",
    "cols = []\n",
    "for i in range(no_of_workers):\n",
    "    cols.append(str(i+1))\n",
    "cols.append('Total Run Duration')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "173f62b9-dfbe-4ab9-ba77-c3534c9251bb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "user_name = os.environ['DOMINO_USER_NAME']\n",
    "project_id = os.environ['DOMINO_PROJECT_ID'] \n",
    "exp_name = f'Exp-{user_name}'\n",
    "exp = None\n",
    "try:\n",
    "    exp = mlflow.get_experiment_by_name(exp_name)    \n",
    "except:\n",
    "    print('Experiment Not Found Create it')\n",
    "    mlflow.create_experiment(exp_name)    \n",
    "    exp = mlflow.get_experiment_by_name(exp_name)    \n",
    "print(exp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6291dae-99ff-41fa-b99c-7f7d34187d83",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "start_time = time.time()\n",
    "results=[]\n",
    "direct_s3_write = True\n",
    "results_s4_direct = ray.get([mlflow_write_remote.remote(exp_name,worker_id,project_id,AWS_ROLE_ARN,direct_s3_write) for worker_id in range(no_of_workers)])\n",
    "duration = time.time() - start_time\n",
    "results_s4_direct.append(str(duration))\n",
    "s4_pd_direct = pd.DataFrame.from_dict({'worker_index': cols, 'durations': results_s4_direct})\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "results=[]\n",
    "direct_s3_write = False\n",
    "results_s4_mlflow = ray.get([mlflow_write_remote.remote(exp_name,worker_id,project_id,AWS_ROLE_ARN,direct_s3_write) for worker_id in range(no_of_workers)])\n",
    "duration = time.time() - start_time\n",
    "results_s4_mlflow.append(str(duration))\n",
    "s4_pd_mlflow = pd.DataFrame.from_dict({'worker_index': cols, 'durations': results_s4_mlflow})\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5546d944-8125-4575-a653-aa4556d7011f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print('Using Boto3 - Explicit boto3 based writes to S3 from Ray Workers')\n",
    "#display(s4_pd_direct)\n",
    "print(s4_pd_direct.to_markdown())\n",
    "print('\\n\\n\\n------------------------------------------------\\n\\n\\n')\n",
    "print('Using MLFLOW client - MLFLOW client uses boto3 to write to S3 from Ray Workers')\n",
    "#display(s4_pd_mlflow)\n",
    "print(s4_pd_mlflow.to_markdown())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da29bbf5-b7fe-4044-a063-aa0f7dfec196",
   "metadata": {},
   "source": [
    "## Scaling Ray Workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "779b2900-47c8-4105-be8e-1fd87a0f6551",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!curl http://rayclusterscaler-svc.domino-field/healthz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e2974b-779e-444d-aae1-ed5c53460287",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "access_token_endpoint='http://localhost:8899/access-token'\n",
    "resp = requests.get(access_token_endpoint)\n",
    "\n",
    "\n",
    "token = resp.text\n",
    "headers = {\n",
    "             \"Content-Type\": \"application/json\",\n",
    "             \"Authorization\": \"Bearer \" + token,\n",
    "        }\n",
    "#Example\n",
    "endpoint='http://rayclusterscaler-svc.domino-field/rayclusterscaler/list'\n",
    "resp = requests.get(endpoint,headers=headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b30b812-58dc-492b-8191-d018ebf810a9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Example\n",
    "replicas=850\n",
    "endpoint='http://rayclusterscaler-svc.domino-field/rayclusterscaler/scale'\n",
    "run_id = os.environ['DOMINO_RUN_ID']\n",
    "ray_cluster_id = f'ray-{run_id}'\n",
    "body = {\n",
    "        \"cluster_name\":ray_cluster_id,\n",
    "        \"replicas\" : replicas\n",
    "    }\n",
    "resp = requests.post(endpoint,headers=headers,json=body)\n"
   ]
  }
 ],
 "metadata": {
  "dca-init": "true",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
